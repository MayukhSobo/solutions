-- Code tested/evaluated on following specs:

	-- 32 GB ram ubuntu unix installation (Amazon m2.x2large EC2 specs)
	-- For safety, I plan to run on 64GB installation (Amazon m2.x4large EC2 specs)
	-- Python 2.7 with following open source packages:
		-- pandas
		-- ntlk (code and corpora)
		-- gensim 
		-- numpy
		-- scipy
	-- Ipython 
	-- R 2.14 with follwoing open source packages:
		-- bigglm
		-- randomForest
	-- HDF5 utliities (not used anywhere in actual code, but is used in some of the imported modules)
	-- 50 GB of free volume space for original data + data generated by script.
	
-- To set up, download all uncompressed competition files to ./sourcedata

-- Submissions are generated by running 'build_data.sh' (in bash) from the root 'wp_deploy' directory.

	-- Logs for each step will be pushed to logs directoy.

	-- Final submissions numbered 01 to 08 will be generated in 'submissions' folder.

	-- Total running time ~33 hours.  

-- This code is UGLY and needs refactoring:

	-- Lots of copy/paste, unnecessary code to support convenience features during data mining.
	
	-- Added features until last minute, so no time to clean this stuff up and retest.
	
	-- If any of this ends up getting used, I'll gladly work with you to refactor, comment, and generally make this less embarrassing.
	
	-- Don't judge me :)
	
-- This script has been (loosely) tested and the uploaded submissions were generated from that test.  

	-- However, some tweaks were made during testing which were not included in 'start to finish' run.   

	-- I am assuming (hoping) competition rules allow minor debugging for unforseen events / untested tweaks during final run.

	-- Possible some minor deviances may occur between uploaded submissions and a new 'start to finish' run on the practice data.

-- If you are trying to run this independently at Kaggle, I have an AMI that would be 'good to go' to test this.  

	-- Contact me at (617) 455-8097 (preferred) or at ecsibleyjr@gmail.com with any questions.